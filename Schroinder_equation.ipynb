{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPO9KZZb+Oa7CiMd2t4Fg+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hWQhbgGlmzeT"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#some plote settings\n","plt.close('all')\n","plt.rcParams{'font.family'} = 'serif'\n","plt.rcParam{'font.serif'} = 'Times New Roman'\n","plt.rcParams{'font.size'} = 10\n","plt.rcParams{'font.dpi'} = 1000"]},{"cell_type":"code","source":["\n","# define the neural newtwork model\n","\n","def create_model():\n","  model = {\n","      'dense1 : tf.keras.layers.Dense(50, activation = 'tanh'),  # 50 neurons and tanh activation function\n","      'dense2 : tf.keras.layers.Dense(50, activation = 'tanh'),\n","      'dense3 : tf.keras.layers.Dense(50, activation = 'tanh'),\n","      'output_layer' = tf.keras.layers.Dense(1)\n","  }\n","  return model\n","\n","#call_model() - this function defines the forward pass of the neural netwrok.\n","#                   it takes as input a dictionary model ()\n","\n","def call_model(model, x):\n","  x= model['dense1'](x)\n","  x= model['dense2'](x)\n","  x= model['dense3'](x)\n","  x= model['output_layer'](x)\n","  return x\n","\n","#model = create_model()\n","#print(model)"],"metadata":{"id":"n8Xqdx9ott2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the differential equation using tf.gradiettape\n","def pde(x, model):\n","  with tf.GradientTape() as tape:\n","    tape.watch(x)\n","    y_pred = call_model(model, x)\n","    u_x = tape.gradient(y_pred, x)\n","    y_xx = tape.gradient(y_x, x)\n","\n","    def tape\n","    return y_xx + np.pi**2 * tf.sin(np.pi * x)  #pde lossc"],"metadata":{"id":"0quFOmbxudAZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the loss function\n","def loss(model, x_bc, y_bc):\n","  res = pde(x, model)\n","  #compute the mean squared error of the boundary conditions\n","  loss_pde = tf.reduce.mean(tf.square(res))\n","  y_bc_pred = call_model(model, x_bc)\n","  loss_bc = tf.reduce_mean(tf.square(y_bc_pred - y_bc))\n","  return loss_pde + loss_bc\n"],"metadata":{"id":"Ehimo-Tkw28B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define the training steps\n","def train_step(model, x, x_bc, y_bc, optimizer):\n","  with tf.GradientTape() as tape:\n","    loss_value = loss(model, x, x_bc, y_bc)\n","    grads = tape.gradient(loss_value, [layer.trainable_variables for layer in model.values()])\n","    #flatten the list of trainable variables\n","    grads = [grad for sublist in grads for grad in sublist]\n","    variables = [var for layer in model.values() for var in layer.trainable_variables]\n","    optimizer.apply_gradients(zip(grads, variables))\n","    return loss_value\n","  #grads = tape.gradient(loss_value, model.trainable_variables)\n","  #optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"metadata":{"id":"b0ntg2kVxyP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#setting up the probles for steps\n","\n","#generate training data\n","x_train = np.linspace(-1, 1, 100).reshape(-1,1)\n","x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n","\n","#bc data\n","x_bc = np.array([[-1, 0], [1, 0]], dtype = np.float32)\n","y_bc = np.array([[0, 0], [0, 0]], dtype = np.float32)\n","x_bc = tf.convert_to_tensor(x_bc, dtype = tf.float32)\n","y_bc = tf.convert_to_tensor(y_bc, dtype = tf.float32)\n","\n","\n","#define the pinn model\n","model = create_model()\n","#define the optimizer with a learning rate scheduler\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate = 0.1,\n","    decay_steps = 10000,\n","    decay_rate = 0.9\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n","#optimizer = tf.keras.optimizers.Adam(learning_rate = 0.1)\n","\n"],"metadata":{"id":"a7OqtmOlykWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the model\n","epochs = 2000\n","for each epoch in range(epochs):\n","  loss_value = train_step(model, x_train, x_bc, y_bc, optimizer)\n","  if epoch % 1000 == 0:\n","    print(f\"Epoch {epoch}, Loss: {loss_value.numpy()}\")"],"metadata":{"id":"eM1pL7WdzxYy"},"execution_count":null,"outputs":[]}]}
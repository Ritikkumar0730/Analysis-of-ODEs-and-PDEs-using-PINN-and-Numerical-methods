{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLSnM6nFpVGCFm7lz/Y6NI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"KHJv9BeR4iUp","executionInfo":{"status":"ok","timestamp":1755879509592,"user_tz":-330,"elapsed":5141,"user":{"displayName":"hrithik kumar","userId":"01382621947295724566"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.fft import fft, ifft, fftfreq\n","import h5py\n","import os\n","from tqdm import tqdm\n","import json\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict, Optional\n","import warnings\n","\n","@dataclass\n","class BurgersConfig:\n","    \"\"\"Configuration for Burgers equation simulation\"\"\"\n","    # Domain\n","    L: float = 2 * np.pi\n","    periodic: bool = True\n","\n","    # Viscosity values\n","    viscosities: List[float] = None\n","\n","    # Spatial discretization\n","    Nx_train: int = 256\n","    Nx_test: int = 512\n","\n","    # Temporal discretization\n","    T_final: float = 1.0\n","    cfl_factor: float = 0.4\n","\n","    # Training data\n","    n_train: int = 500\n","    n_val: int = 100\n","    n_test: int = 100\n","\n","    # Initial condition distribution\n","    fourier_ratio: float = 0.7\n","    step_ratio: float = 0.15\n","    gaussian_ratio: float = 0.15\n","\n","    def __post_init__(self):\n","        if self.viscosities is None:\n","            self.viscosities = [3e-4, 8e-4, 2e-3, 5e-3]\n","\n","        # Validate ratios\n","        total = self.fourier_ratio + self.step_ratio + self.gaussian_ratio\n","        if abs(total - 1.0) > 1e-10:\n","            raise ValueError(f\"IC ratios must sum to 1.0, got {total}\")\n","\n","class BurgersDataGenerator:\n","    \"\"\"Generate high-fidelity training data for Burgers' equation\"\"\"\n","\n","    def __init__(self, config: BurgersConfig):\n","        self.config = config\n","\n","    def generate_initial_condition(self, ic_type: str, Nx: int, seed: int) -> np.ndarray:\n","        \"\"\"Generate initial condition based on type\"\"\"\n","        np.random.seed(seed)\n","        x = np.linspace(0, self.config.L, Nx, endpoint=False)\n","\n","        if ic_type == 'fourier':\n","            return self._generate_fourier_ic(x)\n","        elif ic_type == 'step':\n","            return self._generate_step_ic(x)\n","        elif ic_type == 'gaussian':\n","            return self._generate_gaussian_ic(x)\n","        else:\n","            raise ValueError(f\"Unknown IC type: {ic_type}\")\n","\n","    def _generate_fourier_ic(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"Random Fourier series IC\"\"\"\n","        K = np.random.randint(1, 7)  # K ∈ [1,6]\n","        u = np.zeros_like(x)\n","\n","        for k in range(1, K+1):\n","            a_k = np.random.uniform(0.1, 1.0)\n","            phi_k = np.random.uniform(0, 2*np.pi)\n","            u += a_k * np.sin(k * x + phi_k)\n","\n","        return u\n","\n","    def _generate_step_ic(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"Smoothed step function IC\"\"\"\n","        x0 = np.random.uniform(0.2 * self.config.L, 0.8 * self.config.L)\n","        w = np.random.uniform(0.1, 0.3)\n","        A = np.random.uniform(0.5, 1.5)\n","\n","        return A * np.tanh((x - x0) / w)\n","\n","    def _generate_gaussian_ic(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"Gaussian IC\"\"\"\n","        x0 = np.random.uniform(0.2 * self.config.L, 0.8 * self.config.L)\n","        sigma = np.random.uniform(0.2, 0.6)\n","        A = np.random.uniform(0.5, 1.5)\n","\n","        return A * np.exp(-(x - x0)**2 / (2 * sigma**2))\n","\n","    def solve_burgers_spectral(self, u0: np.ndarray, nu: float, Nx: int) -> Tuple[np.ndarray, np.ndarray, Dict]:\n","        \"\"\"\n","        Solve Burgers equation using pseudo-spectral method with ETDRK4 and 2/3 dealiasing\n","        \"\"\"\n","        # Setup\n","        x = np.linspace(0, self.config.L, Nx, endpoint=False)\n","        dx = self.config.L / Nx\n","\n","        # Wavenumbers with 2/3 dealiasing\n","        k = fftfreq(Nx, dx/(2*np.pi))\n","        k_max = (2/3) * Nx//2\n","        dealias = np.abs(k) <= k_max\n","\n","        # ETDRK4 coefficients\n","        dt_base = self.config.cfl_factor * min(dx / (np.max(np.abs(u0)) + 1e-12),\n","                                              dx**2 / (nu + 1e-12))\n","\n","        # Time stepping\n","        t = 0.0\n","        u = u0.copy()\n","        trajectory = [u.copy()]\n","        times = [t]\n","\n","        # Diagnostic info\n","        diagnostics = {\n","            'cfl_numbers': [],\n","            'energy': [0.5 * np.sum(u**2) * dx],\n","            'max_u': [np.max(np.abs(u))]\n","        }\n","\n","        while t < self.config.T_final:\n","            # Adaptive timestep\n","            max_u = np.max(np.abs(u))\n","            dt_convective = dx / (max_u + 1e-12)\n","            dt_diffusive = dx**2 / (nu + 1e-12)\n","            dt = self.config.cfl_factor * min(dt_convective, dt_diffusive)\n","\n","            # Don't overshoot final time\n","            if t + dt > self.config.T_final:\n","                dt = self.config.T_final - t\n","\n","            # ETDRK4 step\n","            u = self._etdrk4_step(u, dt, nu, k, dealias)\n","\n","            t += dt\n","            trajectory.append(u.copy())\n","            times.append(t)\n","\n","            # Diagnostics\n","            cfl_conv = max_u * dt / dx\n","            cfl_diff = nu * dt / dx**2\n","            diagnostics['cfl_numbers'].append({'convective': cfl_conv, 'diffusive': cfl_diff})\n","            diagnostics['energy'].append(0.5 * np.sum(u**2) * dx)\n","            diagnostics['max_u'].append(max_u)\n","\n","        return np.array(trajectory), np.array(times), diagnostics\n","\n","    def _etdrk4_step(self, u: np.ndarray, dt: float, nu: float, k: np.ndarray, dealias: np.ndarray) -> np.ndarray:\n","        \"\"\"Single ETDRK4 step for Burgers equation\"\"\"\n","        # Fourier transform\n","        u_hat = fft(u)\n","\n","        # Linear operator (diffusion)\n","        L = -nu * k**2\n","\n","        # ETDRK4 coefficients\n","        E = np.exp(dt * L)\n","        E2 = np.exp(dt * L / 2)\n","\n","        # Handle division by zero for L = 0\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            phi1 = np.where(L == 0, dt, (E - 1) / L)\n","            phi2 = np.where(L == 0, dt/2, (E2 - 1) / L)\n","            phi3 = np.where(L == 0, dt/2, (E2 - 1) / L)\n","\n","        # Nonlinear term: -u * du/dx\n","        def N(u_hat):\n","            u_real = np.real(ifft(u_hat))\n","            du_dx = np.real(ifft(1j * k * u_hat))\n","            nonlinear = -u_real * du_dx\n","            N_hat = fft(nonlinear)\n","            # Apply dealiasing\n","            N_hat[~dealias] = 0\n","            return N_hat\n","\n","        # ETDRK4 stages\n","        N1 = N(u_hat)\n","        a = E2 * u_hat + phi2 * N1\n","        N2 = N(a)\n","        b = E2 * u_hat + phi3 * N2\n","        N3 = N(b)\n","        c = E2 * a + phi2 * (2*N3 - N1)\n","        N4 = N(c)\n","\n","        # Final update\n","        u_hat_new = E * u_hat + phi1 * (N1 + 2*N2 + 2*N3 + N4) / 6\n","\n","        return np.real(ifft(u_hat_new))\n","\n","    def generate_dataset(self, output_dir: str = \"burgers_dataset\") -> Dict:\n","        \"\"\"Generate complete dataset for CNN/TCN training\"\"\"\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Determine IC types for each trajectory\n","        def get_ic_distribution(n_total):\n","            n_fourier = int(n_total * self.config.fourier_ratio)\n","            n_step = int(n_total * self.config.step_ratio)\n","            n_gaussian = n_total - n_fourier - n_step\n","\n","            ic_types = (['fourier'] * n_fourier +\n","                       ['step'] * n_step +\n","                       ['gaussian'] * n_gaussian)\n","            np.random.shuffle(ic_types)\n","            return ic_types\n","\n","        # Generate seeds and parameters\n","        np.random.seed(42)  # For reproducibility\n","\n","        datasets = {}\n","\n","        for split, n_traj in [('train', self.config.n_train),\n","                             ('val', self.config.n_val),\n","                             ('test', self.config.n_test)]:\n","\n","            print(f\"Generating {split} dataset ({n_traj} trajectories)...\")\n","\n","            # Use different Nx for test split\n","            Nx = self.config.Nx_test if split == 'test' else self.config.Nx_train\n","\n","            # Get IC types\n","            ic_types = get_ic_distribution(n_traj)\n","\n","            # Generate unique seeds for this split\n","            base_seed = {'train': 1000, 'val': 2000, 'test': 3000}[split]\n","            seeds = list(range(base_seed, base_seed + n_traj))\n","\n","            # Randomly assign viscosities\n","            viscosities = np.random.choice(self.config.viscosities, n_traj)\n","\n","            # For test set, ensure we have at least one unseen viscosity\n","            if split == 'test':\n","                # Add an OOD viscosity\n","                ood_nu = 1e-3  # Between our training values\n","                viscosities[0] = ood_nu\n","\n","            # Generate trajectories\n","            X_all = []\n","            y_all = []\n","            metadata = []\n","\n","            for i, (ic_type, seed, nu) in enumerate(tqdm(zip(ic_types, seeds, viscosities),\n","                                                        total=n_traj,\n","                                                        desc=f\"{split.capitalize()}\")):\n","\n","                # Generate initial condition\n","                u0 = self.generate_initial_condition(ic_type, Nx, seed)\n","\n","                # Solve PDE\n","                trajectory, times, diagnostics = self.solve_burgers_spectral(u0, nu, Nx)\n","\n","                # Create sliding window samples\n","                for t_idx in range(len(trajectory) - 1):\n","                    X_all.append(trajectory[t_idx])\n","                    y_all.append(trajectory[t_idx + 1])\n","\n","                    metadata.append({\n","                        'trajectory_id': i,\n","                        'time_step': t_idx,\n","                        'time': times[t_idx],\n","                        'viscosity': nu,\n","                        'ic_type': ic_type,\n","                        'ic_seed': seed,\n","                        'Nx': Nx,\n","                        'max_cfl_conv': max([d['convective'] for d in diagnostics['cfl_numbers']]),\n","                        'max_cfl_diff': max([d['diffusive'] for d in diagnostics['cfl_numbers']])\n","                    })\n","\n","            X = np.array(X_all)\n","            y = np.array(y_all)\n","\n","            # Store dataset\n","            datasets[split] = {\n","                'X': X,\n","                'y': y,\n","                'metadata': metadata\n","            }\n","\n","            print(f\"{split}: {X.shape[0]} samples, shape {X.shape}\")\n","\n","        # Compute global normalization statistics from training data\n","        X_train = datasets['train']['X']\n","        y_train = datasets['train']['y']\n","        all_data = np.concatenate([X_train.flatten(), y_train.flatten()])\n","\n","        norm_stats = {\n","            'mean': float(np.mean(all_data)),\n","            'std': float(np.std(all_data))\n","        }\n","\n","        print(f\"Normalization stats - Mean: {norm_stats['mean']:.6f}, Std: {norm_stats['std']:.6f}\")\n","\n","        # Save datasets\n","        for split in datasets:\n","            # Save as HDF5\n","            with h5py.File(os.path.join(output_dir, f\"{split}.h5\"), 'w') as f:\n","                f.create_dataset('X', data=datasets[split]['X'])\n","                f.create_dataset('y', data=datasets[split]['y'])\n","\n","                # Save metadata as JSON strings\n","                metadata_json = [json.dumps(m) for m in datasets[split]['metadata']]\n","                f.create_dataset('metadata', data=metadata_json, dtype=h5py.string_dtype())\n","\n","        # Save configuration and normalization stats\n","        config_dict = {\n","            'config': {\n","                'L': self.config.L,\n","                'viscosities': self.config.viscosities,\n","                'Nx_train': self.config.Nx_train,\n","                'Nx_test': self.config.Nx_test,\n","                'T_final': self.config.T_final,\n","                'cfl_factor': self.config.cfl_factor,\n","                'n_train': self.config.n_train,\n","                'n_val': self.config.n_val,\n","                'n_test': self.config.n_test,\n","                'fourier_ratio': self.config.fourier_ratio,\n","                'step_ratio': self.config.step_ratio,\n","                'gaussian_ratio': self.config.gaussian_ratio\n","            },\n","            'normalization': norm_stats\n","        }\n","\n","        with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n","            json.dump(config_dict, f, indent=2)\n","\n","        # Generate quality check plots\n","        self._generate_quality_plots(datasets, output_dir, norm_stats)\n","\n","        return datasets, norm_stats\n","\n","    def _generate_quality_plots(self, datasets: Dict, output_dir: str, norm_stats: Dict):\n","        \"\"\"Generate quality check visualizations\"\"\"\n","\n","        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n","\n","        # Plot 1: Sample trajectories\n","        ax = axes[0, 0]\n","        train_data = datasets['train']\n","\n","        # Find a few representative trajectories\n","        for traj_id in [0, 1, 2]:\n","            traj_samples = [m for m in train_data['metadata'] if m['trajectory_id'] == traj_id]\n","            if traj_samples:\n","                times = [m['time'] for m in traj_samples]\n","                energies = []\n","\n","                for i, m in enumerate(traj_samples):\n","                    idx = next(j for j, meta in enumerate(train_data['metadata']) if meta == m)\n","                    u = train_data['X'][idx]\n","                    energy = 0.5 * np.sum(u**2) * (2*np.pi / len(u))\n","                    energies.append(energy)\n","\n","                ax.plot(times, energies, 'o-', alpha=0.7, label=f'Traj {traj_id}, ν={traj_samples[0][\"viscosity\"]:.1e}')\n","\n","        ax.set_xlabel('Time')\n","        ax.set_ylabel('Energy')\n","        ax.set_title('Energy Decay (Quality Check)')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","        # Plot 2: CFL number distribution\n","        ax = axes[0, 1]\n","        cfl_conv = [m['max_cfl_conv'] for m in train_data['metadata']]\n","        cfl_diff = [m['max_cfl_diff'] for m in train_data['metadata']]\n","\n","        ax.hist(cfl_conv, bins=20, alpha=0.5, label='Convective CFL', density=True)\n","        ax.hist(cfl_diff, bins=20, alpha=0.5, label='Diffusive CFL', density=True)\n","        ax.axvline(1.0, color='red', linestyle='--', alpha=0.7, label='CFL = 1')\n","        ax.set_xlabel('CFL Number')\n","        ax.set_ylabel('Density')\n","        ax.set_title('CFL Number Distribution')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","        # Plot 3: Data distribution\n","        ax = axes[1, 0]\n","        all_X = np.concatenate([datasets[split]['X'].flatten() for split in datasets])\n","        ax.hist(all_X, bins=50, alpha=0.7, density=True)\n","        ax.axvline(norm_stats['mean'], color='red', linestyle='--', label=f\"Mean = {norm_stats['mean']:.3f}\")\n","        ax.axvline(norm_stats['mean'] - norm_stats['std'], color='orange', linestyle=':', label=f\"±1σ\")\n","        ax.axvline(norm_stats['mean'] + norm_stats['std'], color='orange', linestyle=':', alpha=0.7)\n","        ax.set_xlabel('Value')\n","        ax.set_ylabel('Density')\n","        ax.set_title('Data Distribution')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","        # Plot 4: Sample initial conditions\n","        ax = axes[1, 1]\n","        x_train = np.linspace(0, 2*np.pi, self.config.Nx_train, endpoint=False)\n","\n","        # Show one of each IC type\n","        ic_examples = {}\n","        for m in train_data['metadata'][:50]:  # Check first 50\n","            if m['time_step'] == 0 and m['ic_type'] not in ic_examples:\n","                idx = next(j for j, meta in enumerate(train_data['metadata']) if meta == m)\n","                ic_examples[m['ic_type']] = train_data['X'][idx]\n","\n","        for ic_type, u0 in ic_examples.items():\n","            ax.plot(x_train, u0, label=f'{ic_type.capitalize()} IC', linewidth=2)\n","\n","        ax.set_xlabel('x')\n","        ax.set_ylabel('u(x, t=0)')\n","        ax.set_title('Sample Initial Conditions')\n","        ax.legend()\n","        ax.grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(output_dir, 'quality_checks.png'), dpi=150, bbox_inches='tight')\n","        plt.close()\n","\n","        print(f\"Quality check plots saved to {output_dir}/quality_checks.png\")\n","\n","# Example usage and data loading utilities\n","class BurgersDataLoader:\n","    \"\"\"Utility for loading and preprocessing the generated dataset\"\"\"\n","\n","    def __init__(self, dataset_dir: str):\n","        self.dataset_dir = dataset_dir\n","\n","        # Load configuration and normalization stats\n","        with open(os.path.join(dataset_dir, 'config.json'), 'r') as f:\n","            self.config_data = json.load(f)\n","\n","        self.norm_stats = self.config_data['normalization']\n","\n","    def load_split(self, split: str) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n","        \"\"\"Load a specific split of the dataset\"\"\"\n","        with h5py.File(os.path.join(self.dataset_dir, f\"{split}.h5\"), 'r') as f:\n","            X = f['X'][:]\n","            y = f['y'][:]\n","            metadata = [json.loads(m.decode()) for m in f['metadata'][:]]\n","\n","        return X, y, metadata\n","\n","    def normalize(self, data: np.ndarray) -> np.ndarray:\n","        \"\"\"Apply normalization\"\"\"\n","        return (data - self.norm_stats['mean']) / self.norm_stats['std']\n","\n","    def denormalize(self, data: np.ndarray) -> np.ndarray:\n","        \"\"\"Remove normalization\"\"\"\n","        return data * self.norm_stats['std'] + self.norm_stats['mean']\n","\n","    def get_cnn_format(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"Convert to CNN format: [batch, channels=1, spatial]\"\"\"\n","        return X[:, np.newaxis, :]\n","\n","    def get_tcn_format(self, X: np.ndarray, window_size: int = 3) -> np.ndarray:\n","        \"\"\"Convert to TCN format with temporal stacking\"\"\"\n","        # This would require modifying the data generation to create temporal windows\n","        # For now, return CNN format\n","        return self.get_cnn_format(X)\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Create configuration with specified parameters\n","    config = BurgersConfig(\n","        viscosities=[3e-4, 8e-4, 2e-3, 5e-3],\n","        Nx_train=256,\n","        Nx_test=512,\n","        T_final=1.0,\n","        n_train=500,\n","        n_val=100,\n","        n_test=100,\n","        fourier_ratio=0.7,\n","        step_ratio=0.15,\n","        gaussian_ratio=0.15\n","    )\n","\n","    # Generate dataset\n","    generator = BurgersDataGenerator(config)\n","    datasets, norm_stats = generator.generate_dataset(\"burgers_dataset\")\n","\n","    print(\"\\nDataset generation complete!\")\n","    print(f\"Training samples: {datasets['train']['X'].shape[0]}\")\n","    print(f\"Validation samples: {datasets['val']['X'].shape[0]}\")\n","    print(f\"Test samples: {datasets['test']['X'].shape[0]}\")\n","    print(f\"Normalization - Mean: {norm_stats['mean']:.6f}, Std: {norm_stats['std']:.6f}\")\n","\n","    # Example of loading and using the data\n","    print(\"\\nExample usage:\")\n","    loader = BurgersDataLoader(\"burgers_dataset\")\n","    X_train, y_train, metadata_train = loader.load_split('train')\n","\n","    # Normalize for training\n","    X_train_norm = loader.normalize(X_train)\n","    y_train_norm = loader.normalize(y_train)\n","\n","    # Convert to CNN format\n","    X_cnn = loader.get_cnn_format(X_train_norm)\n","    print(f\"CNN format shape: {X_cnn.shape}\")  # [batch, 1, Nx]"],"metadata":{"id":"OS2js_E74OBy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755879558619,"user_tz":-330,"elapsed":43733,"user":{"displayName":"hrithik kumar","userId":"01382621947295724566"}},"outputId":"6da9412c-13ae-485d-b65c-295afe96f2c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating train dataset (500 trajectories)...\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100%|██████████| 500/500 [00:20<00:00, 24.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train: 70166 samples, shape (70166, 256)\n","Generating val dataset (100 trajectories)...\n"]},{"output_type":"stream","name":"stderr","text":["Val: 100%|██████████| 100/100 [00:03<00:00, 25.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val: 14050 samples, shape (14050, 256)\n","Generating test dataset (100 trajectories)...\n"]},{"output_type":"stream","name":"stderr","text":["Test: 100%|██████████| 100/100 [00:09<00:00, 10.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["test: 27203 samples, shape (27203, 512)\n","Normalization stats - Mean: 0.010787, Std: 0.805432\n","Quality check plots saved to burgers_dataset/quality_checks.png\n","\n","Dataset generation complete!\n","Training samples: 70166\n","Validation samples: 14050\n","Test samples: 27203\n","Normalization - Mean: 0.010787, Std: 0.805432\n","\n","Example usage:\n","CNN format shape: (70166, 1, 256)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import json\n","import os\n","from tqdm import tqdm\n","from typing import Dict, List, Tuple, Optional\n","import time\n","from dataclasses import dataclass\n","\n","@dataclass\n","class TrainingConfig:\n","    \"\"\"Configuration for CNN training\"\"\"\n","    # Model architecture\n","    hidden_channels: List[int] = None\n","    kernel_sizes: List[int] = None\n","    activation: str = 'relu'\n","    dropout_rate: float = 0.1\n","\n","    # Training parameters\n","    batch_size: int = 64\n","    learning_rate: float = 1e-3\n","    num_epochs: int = 100\n","    weight_decay: float = 1e-5\n","\n","    # Learning rate scheduling\n","    lr_patience: int = 10\n","    lr_factor: float = 0.5\n","\n","    # Early stopping\n","    early_stopping_patience: int = 20\n","\n","    # Device\n","    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    # Logging\n","    log_interval: int = 100\n","    save_dir: str = 'cnn_models'\n","\n","    def __post_init__(self):\n","        if self.hidden_channels is None:\n","            self.hidden_channels = [16, 32, 64, 32, 16]\n","        if self.kernel_sizes is None:\n","            self.kernel_sizes = [7, 5, 5, 5, 7]\n","\n","class BurgersDataset(Dataset):\n","    \"\"\"PyTorch Dataset for your generated Burgers equation data\"\"\"\n","\n","    def __init__(self, X: np.ndarray, y: np.ndarray, metadata: List[Dict],\n","                 normalize_fn=None, device='cpu'):\n","        self.X = torch.FloatTensor(X).to(device)\n","        self.y = torch.FloatTensor(y).to(device)\n","        self.metadata = metadata\n","        self.normalize_fn = normalize_fn\n","\n","        # Convert to CNN format: [batch, channels=1, spatial]\n","        if len(self.X.shape) == 2:\n","            self.X = self.X.unsqueeze(1)  # Add channel dimension\n","        if len(self.y.shape) == 2:\n","            self.y = self.y.unsqueeze(1)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","class BurgersCNN(nn.Module):\n","    \"\"\"1D CNN for Burgers equation surrogate modeling\"\"\"\n","\n","    def __init__(self, config: TrainingConfig):\n","        super().__init__()\n","        self.config = config\n","\n","        # Build encoder-decoder architecture\n","        layers = []\n","        in_channels = 1  # Single field u(x,t)\n","\n","        # Encoder-decoder with skip connections\n","        for i, (out_channels, kernel_size) in enumerate(zip(config.hidden_channels, config.kernel_sizes)):\n","            layers.extend([\n","                nn.Conv1d(in_channels, out_channels, kernel_size,\n","                         padding=kernel_size//2, bias=True),\n","                self._get_activation(),\n","                nn.BatchNorm1d(out_channels),\n","                nn.Dropout1d(config.dropout_rate) if i > 0 else nn.Identity(),\n","            ])\n","            in_channels = out_channels\n","\n","        # Final output layer\n","        layers.append(nn.Conv1d(in_channels, 1, kernel_size=1, padding=0))\n","\n","        self.network = nn.Sequential(*layers)\n","\n","        # Initialize weights\n","        self._init_weights()\n","\n","    def _get_activation(self):\n","        if self.config.activation == 'relu':\n","            return nn.ReLU(inplace=True)\n","        elif self.config.activation == 'gelu':\n","            return nn.GELU()\n","        elif self.config.activation == 'swish':\n","            return nn.SiLU()\n","        else:\n","            return nn.ReLU(inplace=True)\n","\n","    def _init_weights(self):\n","        \"\"\"Initialize weights with Xavier/Glorot initialization\"\"\"\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv1d):\n","                nn.init.xavier_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.BatchNorm1d):\n","                nn.init.ones_(m.weight)\n","                nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass\"\"\"\n","        return self.network(x)\n","\n","class BurgersTrainer:\n","    \"\"\"Training pipeline for Burgers CNN using your generated data\"\"\"\n","\n","    def __init__(self, config: TrainingConfig, dataset_dir: str = \"burgers_dataset\"):\n","        self.config = config\n","        self.dataset_dir = dataset_dir\n","\n","        # Create save directory\n","        os.makedirs(config.save_dir, exist_ok=True)\n","\n","        # Load normalization stats from your generated data\n","        with open(os.path.join(dataset_dir, '/content/sample_data/anscombe.json'), 'r') as f:\n","            dataset_config = json.load(f)\n","\n","        self.norm_stats = dataset_config['normalization']\n","        print(f\"Loaded normalization stats - Mean: {self.norm_stats['mean']:.6f}, Std: {self.norm_stats['std']:.6f}\")\n","\n","        # Load datasets\n","        self.datasets = self._load_datasets()\n","\n","        # Create data loaders\n","        self.dataloaders = self._create_dataloaders()\n","\n","        # Initialize model\n","        self.model = BurgersCNN(config).to(config.device)\n","\n","        # Loss function and optimizer\n","        self.criterion = nn.MSELoss()\n","        self.optimizer = optim.Adam(self.model.parameters(),\n","                                   lr=config.learning_rate,\n","                                   weight_decay=config.weight_decay)\n","\n","        # Learning rate scheduler\n","        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, mode='min', factor=config.lr_factor,\n","            patience=config.lr_patience, verbose=True\n","        )\n","\n","        # Training history\n","        self.history = {\n","            'train_loss': [],\n","            'val_loss': [],\n","            'learning_rates': [],\n","            'epoch_times': []\n","        }\n","\n","        print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n","        print(f\"Using device: {config.device}\")\n","\n","    def _load_datasets(self) -> Dict[str, Tuple[np.ndarray, np.ndarray, List[Dict]]]:\n","        \"\"\"Load your generated datasets\"\"\"\n","        datasets = {}\n","\n","        for split in ['train', 'val', 'test']:\n","            with h5py.File(os.path.join(self.dataset_dir, f\"{split}.h5\"), 'r') as f:\n","                X = f['X'][:]\n","                y = f['y'][:]\n","                metadata = [json.loads(m.decode()) for m in f['metadata'][:]]\n","\n","            # Normalize data\n","            X_norm = (X - self.norm_stats['mean']) / self.norm_stats['std']\n","            y_norm = (y - self.norm_stats['mean']) / self.norm_stats['std']\n","\n","            datasets[split] = (X_norm, y_norm, metadata)\n","            print(f\"Loaded {split}: {X.shape[0]} samples, shape {X.shape}\")\n","\n","        return datasets\n","\n","    def _create_dataloaders(self) -> Dict[str, DataLoader]:\n","        \"\"\"Create PyTorch data loaders\"\"\"\n","        dataloaders = {}\n","\n","        for split in ['train', 'val', 'test']:\n","            X, y, metadata = self.datasets[split]\n","\n","            dataset = BurgersDataset(X, y, metadata, device=self.config.device)\n","\n","            shuffle = (split == 'train')\n","            batch_size = self.config.batch_size if split != 'test' else min(64, len(X))\n","\n","            dataloaders[split] = DataLoader(\n","                dataset, batch_size=batch_size, shuffle=shuffle,\n","                num_workers=0, pin_memory=False  # Set to 0 since data is already on device\n","            )\n","\n","        return dataloaders\n","\n","    def train_epoch(self) -> float:\n","        \"\"\"Train for one epoch\"\"\"\n","        self.model.train()\n","        total_loss = 0.0\n","        num_batches = 0\n","\n","        pbar = tqdm(self.dataloaders['train'], desc='Training', leave=False)\n","        for batch_idx, (inputs, targets) in enumerate(pbar):\n","            # Forward pass\n","            outputs = self.model(inputs)\n","            loss = self.criterion(outputs, targets)\n","\n","            # Backward pass\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","\n","            # Gradient clipping (helps with stability)\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","\n","            self.optimizer.step()\n","\n","            # Update metrics\n","            total_loss += loss.item()\n","            num_batches += 1\n","\n","            # Update progress bar\n","            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n","\n","            # Log intermediate results\n","            if batch_idx % self.config.log_interval == 0:\n","                pbar.set_description(f'Training (batch {batch_idx}/{len(self.dataloaders[\"train\"])})')\n","\n","        return total_loss / num_batches\n","\n","    def validate_epoch(self) -> float:\n","        \"\"\"Validate for one epoch\"\"\"\n","        self.model.eval()\n","        total_loss = 0.0\n","        num_batches = 0\n","\n","        with torch.no_grad():\n","            for inputs, targets in tqdm(self.dataloaders['val'], desc='Validating', leave=False):\n","                outputs = self.model(inputs)\n","                loss = self.criterion(outputs, targets)\n","\n","                total_loss += loss.item()\n","                num_batches += 1\n","\n","        return total_loss / num_batches\n","\n","    def train(self):\n","        \"\"\"Full training loop\"\"\"\n","        print(f\"\\nStarting training for {self.config.num_epochs} epochs...\")\n","        print(\"=\" * 60)\n","\n","        best_val_loss = float('inf')\n","        patience_counter = 0\n","\n","        for epoch in range(self.config.num_epochs):\n","            epoch_start = time.time()\n","\n","            # Train and validate\n","            train_loss = self.train_epoch()\n","            val_loss = self.validate_epoch()\n","\n","            # Update learning rate\n","            self.scheduler.step(val_loss)\n","            current_lr = self.optimizer.param_groups[0]['lr']\n","\n","            # Record history\n","            epoch_time = time.time() - epoch_start\n","            self.history['train_loss'].append(train_loss)\n","            self.history['val_loss'].append(val_loss)\n","            self.history['learning_rates'].append(current_lr)\n","            self.history['epoch_times'].append(epoch_time)\n","\n","            # Print progress\n","            print(f\"Epoch {epoch+1:3d}/{self.config.num_epochs} | \"\n","                  f\"Train Loss: {train_loss:.6f} | \"\n","                  f\"Val Loss: {val_loss:.6f} | \"\n","                  f\"LR: {current_lr:.2e} | \"\n","                  f\"Time: {epoch_time:.1f}s\")\n","\n","            # Early stopping and model saving\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                patience_counter = 0\n","\n","                # Save best model\n","                self.save_model('best_model.pth', epoch, val_loss)\n","                print(f\"  → New best model saved (val_loss: {val_loss:.6f})\")\n","            else:\n","                patience_counter += 1\n","\n","            # Early stopping check\n","            if patience_counter >= self.config.early_stopping_patience:\n","                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n","                break\n","\n","            # Save checkpoint every 10 epochs\n","            if (epoch + 1) % 10 == 0:\n","                self.save_model(f'checkpoint_epoch_{epoch+1}.pth', epoch, val_loss)\n","\n","        print(\"=\" * 60)\n","        print(f\"Training completed! Best validation loss: {best_val_loss:.6f}\")\n","\n","        # Plot training history\n","        self.plot_training_history()\n","\n","    def save_model(self, filename: str, epoch: int, val_loss: float):\n","        \"\"\"Save model checkpoint\"\"\"\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'scheduler_state_dict': self.scheduler.state_dict(),\n","            'val_loss': val_loss,\n","            'config': self.config,\n","            'norm_stats': self.norm_stats,\n","            'history': self.history\n","        }\n","\n","        torch.save(checkpoint, os.path.join(self.config.save_dir, filename))\n","\n","    def load_model(self, filename: str):\n","        \"\"\"Load model checkpoint\"\"\"\n","        checkpoint = torch.load(os.path.join(self.config.save_dir, filename),\n","                               map_location=self.config.device)\n","\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        self.history = checkpoint['history']\n","\n","        print(f\"Model loaded from epoch {checkpoint['epoch']} with val_loss: {checkpoint['val_loss']:.6f}\")\n","\n","        return checkpoint\n","\n","    def test_model(self) -> Dict[str, float]:\n","        \"\"\"Test the best model\"\"\"\n","        # Load best model\n","        self.load_model('best_model.pth')\n","\n","        self.model.eval()\n","        test_metrics = {'mse': 0.0, 'mae': 0.0, 'max_error': 0.0}\n","        num_samples = 0\n","\n","        with torch.no_grad():\n","            for inputs, targets in tqdm(self.dataloaders['test'], desc='Testing'):\n","                outputs = self.model(inputs)\n","\n","                # Denormalize for physical interpretation\n","                outputs_denorm = outputs * self.norm_stats['std'] + self.norm_stats['mean']\n","                targets_denorm = targets * self.norm_stats['std'] + self.norm_stats['mean']\n","\n","                # Compute metrics\n","                mse = torch.mean((outputs_denorm - targets_denorm)**2)\n","                mae = torch.mean(torch.abs(outputs_denorm - targets_denorm))\n","                max_err = torch.max(torch.abs(outputs_denorm - targets_denorm))\n","\n","                test_metrics['mse'] += mse.item() * inputs.size(0)\n","                test_metrics['mae'] += mae.item() * inputs.size(0)\n","                test_metrics['max_error'] = max(test_metrics['max_error'], max_err.item())\n","\n","                num_samples += inputs.size(0)\n","\n","        # Average metrics\n","        test_metrics['mse'] /= num_samples\n","        test_metrics['mae'] /= num_samples\n","        test_metrics['rmse'] = np.sqrt(test_metrics['mse'])\n","\n","        print(\"\\nTest Results:\")\n","        print(f\"  RMSE: {test_metrics['rmse']:.6f}\")\n","        print(f\"  MAE:  {test_metrics['mae']:.6f}\")\n","        print(f\"  Max Error: {test_metrics['max_error']:.6f}\")\n","\n","        return test_metrics\n","\n","    def plot_training_history(self):\n","        \"\"\"Plot training and validation loss\"\"\"\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","        epochs = range(1, len(self.history['train_loss']) + 1)\n","\n","        # Loss plot\n","        ax1.plot(epochs, self.history['train_loss'], 'b-', label='Training Loss')\n","        ax1.plot(epochs, self.history['val_loss'], 'r-', label='Validation Loss')\n","        ax1.set_xlabel('Epoch')\n","        ax1.set_ylabel('Loss')\n","        ax1.set_title('Training and Validation Loss')\n","        ax1.legend()\n","        ax1.grid(True, alpha=0.3)\n","        ax1.set_yscale('log')\n","\n","        # Learning rate plot\n","        ax2.plot(epochs, self.history['learning_rates'], 'g-', label='Learning Rate')\n","        ax2.set_xlabel('Epoch')\n","        ax2.set_ylabel('Learning Rate')\n","        ax2.set_title('Learning Rate Schedule')\n","        ax2.legend()\n","        ax2.grid(True, alpha=0.3)\n","        ax2.set_yscale('log')\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(self.config.save_dir, 'training_history.png'),\n","                   dpi=150, bbox_inches='tight')\n","        plt.show()\n","\n","        print(f\"Training history plot saved to {self.config.save_dir}/training_history.png\")\n","\n","    def visualize_predictions(self, num_samples: int = 5):\n","        \"\"\"Visualize model predictions vs ground truth\"\"\"\n","        self.load_model('best_model.pth')\n","        self.model.eval()\n","\n","        # Get some test samples\n","        test_dataset = self.datasets['test']\n","        X_test, y_test, metadata_test = test_dataset\n","\n","        # Select random samples\n","        indices = np.random.choice(len(X_test), num_samples, replace=False)\n","\n","        fig, axes = plt.subplots(num_samples, 1, figsize=(12, 3*num_samples))\n","        if num_samples == 1:\n","            axes = [axes]\n","\n","        with torch.no_grad():\n","            for i, idx in enumerate(indices):\n","                # Get data\n","                x_sample = torch.FloatTensor(X_test[idx]).unsqueeze(0).unsqueeze(0).to(self.config.device)\n","                y_true = y_test[idx]\n","\n","                # Predict\n","                y_pred = self.model(x_sample).squeeze().cpu().numpy()\n","\n","                # Denormalize\n","                y_true_denorm = y_true * self.norm_stats['std'] + self.norm_stats['mean']\n","                y_pred_denorm = y_pred * self.norm_stats['std'] + self.norm_stats['mean']\n","                x_denorm = X_test[idx] * self.norm_stats['std'] + self.norm_stats['mean']\n","\n","                # Plot\n","                x_grid = np.linspace(0, 2*np.pi, len(y_true_denorm))\n","\n","                axes[i].plot(x_grid, y_true_denorm, 'b-', linewidth=2, label='Ground Truth', alpha=0.8)\n","                axes[i].plot(x_grid, y_pred_denorm, 'r--', linewidth=2, label='CNN Prediction')\n","                axes[i].plot(x_grid, x_denorm, 'g:', linewidth=1, label='Input', alpha=0.6)\n","\n","                # Add metadata info\n","                meta = metadata_test[idx]\n","                axes[i].set_title(f\"Sample {i+1}: ν={meta['viscosity']:.1e}, \"\n","                                f\"t={meta['time']:.3f}, IC: {meta['ic_type']}\")\n","                axes[i].set_xlabel('x')\n","                axes[i].set_ylabel('u(x,t)')\n","                axes[i].legend()\n","                axes[i].grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(self.config.save_dir, 'predictions_visualization.png'),\n","                   dpi=150, bbox_inches='tight')\n","        plt.show()\n","\n","        print(f\"Predictions visualization saved to {self.config.save_dir}/predictions_visualization.png\")\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Configure training\n","    config = TrainingConfig(\n","        # Model architecture\n","        hidden_channels=[32, 64, 128, 64, 32],\n","        kernel_sizes=[7, 5, 5, 5, 7],\n","        activation='gelu',\n","        dropout_rate=0.1,\n","\n","        # Training parameters\n","        batch_size=128,\n","        learning_rate=1e-3,\n","        num_epochs=200,\n","        weight_decay=1e-5,\n","\n","        # Learning rate scheduling\n","        lr_patience=15,\n","        lr_factor=0.7,\n","\n","        # Early stopping\n","        early_stopping_patience=30,\n","\n","        # Other settings\n","        log_interval=50,\n","        save_dir='burgers_cnn_models'\n","    )\n","\n","    # Initialize trainer (uses your generated data)\n","    trainer = BurgersTrainer(config, dataset_dir=\"burgers_dataset\")\n","\n","    print(f\"Dataset info:\")\n","    print(f\"  Training samples: {len(trainer.datasets['train'][0]):,}\")\n","    print(f\"  Validation samples: {len(trainer.datasets['val'][0]):,}\")\n","    print(f\"  Test samples: {len(trainer.datasets['test'][0]):,}\")\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Test the trained model\n","    test_metrics = trainer.test_model()\n","\n","    # Create visualizations\n","    trainer.visualize_predictions(num_samples=6)\n","\n","    print(\"\\nTraining complete! Check the 'burgers_cnn_models' directory for:\")\n","    print(\"  - best_model.pth (trained model)\")\n","    print(\"  - training_history.png (loss curves)\")\n","    print(\"  - predictions_visualization.png (sample predictions)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"3ddEZPcZqZH-","executionInfo":{"status":"error","timestamp":1755881472070,"user_tz":-330,"elapsed":330,"user":{"displayName":"hrithik kumar","userId":"01382621947295724566"}},"outputId":"9ca6ee98-b231-4370-8aef-2a46b8eefd59"},"execution_count":2,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"list indices must be integers or slices, not str","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2492745097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;31m# Initialize trainer (uses your generated data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBurgersTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"burgers_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset info:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2492745097.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, dataset_dir)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mdataset_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalization'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded normalization stats - Mean: {self.norm_stats['mean']:.6f}, Std: {self.norm_stats['std']:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-cwdCMyZqibS"},"execution_count":null,"outputs":[]}]}